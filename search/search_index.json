{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"AroFlow Documentation","text":"<p>Welcome to AroFlow - a modern workflow orchestration framework that lets you define workflows once and run them anywhere.</p>"},{"location":"#what-is-aroflow","title":"What is AroFlow?","text":"<p>AroFlow is a powerful workflow orchestration tool designed to streamline and automate complex processes across various execution environments. Whether you're managing data pipelines, automating deployments, or coordinating tasks across distributed systems, AroFlow provides a unified platform to define, execute, and monitor your workflows with ease and reliability.</p>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>Backend Agnostic: Define workflows once, run on multiple backends (in-memory, Temporal, Celery, etc.)</li> <li>Plugin System: Extensible architecture with custom plugins for any operation</li> <li>Simple API: Clean, intuitive client API for defining and running workflows</li> <li>Advanced Workflows: Support for sequential, parallel, and map operations</li> <li>Real-time Monitoring: Built-in support for monitoring and logging</li> <li>Error Handling: Robust error handling with retries and graceful failures</li> </ul>"},{"location":"#quick-start","title":"Quick Start","text":"<p>Let's get you up and running with AroFlow in just a few minutes!</p>"},{"location":"#installation","title":"Installation","text":"<pre><code>pip install aroflow\n</code></pre>"},{"location":"#your-first-workflow","title":"Your First Workflow","text":"<p>Here's a simple example to get you started:</p> <pre><code>import aroflow\n\n# 1. Define a Plugin\nclass SayHelloPlugin(aroflow.PluginMixin):\n    plugin_name = \"say_hello\"\n\n    def execute(self, name: str) -&gt; str:\n        return f\"Hello, {name}!\"\n\n# 2. Create a client and register your plugin\nclient = aroflow.create(aroflow.BackendType.IN_MEMORY)\nclient.plugin(SayHelloPlugin)\n\n# 3. Define a workflow\nworkflow = {\n    \"steps\": [\n        {\n            \"id\": \"step1\",\n            \"kind\": \"operation\", \n            \"operation\": \"say_hello\",\n            \"parameters\": {\"name\": \"AroFlow\"},\n        },\n    ]\n}\n\n# 4. Execute the workflow\nresult = client.run(workflow)\nprint(result.to_yaml())\n</code></pre> <p>Output: <pre><code>id: workflow\nstatus: success\nresults:\n- id: step1\n  kind: operation\n  operation: say_hello\n  parameters:\n    name: AroFlow\n  result: Hello, AroFlow!\n  status: success\n  error: null\nerror: null\n</code></pre></p>"},{"location":"#core-concepts","title":"Core Concepts","text":""},{"location":"#plugins","title":"Plugins","text":"<p>Plugins are the building blocks of AroFlow. They encapsulate the business logic that your workflows will execute. Each plugin implements a single <code>execute</code> method and declares a unique <code>plugin_name</code>.</p>"},{"location":"#workflows","title":"Workflows","text":"<p>Workflows are declarative definitions of the tasks you want to execute. They consist of one or more steps that can run sequentially, in parallel, or mapped over data.</p>"},{"location":"#steps","title":"Steps","text":"<p>Steps are individual units of work within a workflow. AroFlow supports three types of steps:</p> <ul> <li>Operation Steps: Execute a single plugin</li> <li>Map Steps: Execute a plugin over a list of inputs</li> <li>Parallel Steps: Execute multiple plugins concurrently</li> </ul>"},{"location":"#backends","title":"Backends","text":"<p>Backends determine where and how your workflows execute. AroFlow abstracts the execution environment, so you can switch backends without changing your workflow definitions.</p>"},{"location":"#next-steps","title":"Next Steps","text":"<p>Ready to dive deeper? Check out our comprehensive guides:</p> <ul> <li>Writing Plugins - Learn how to create custom plugins for your business logic</li> <li>Creating Workflows - Master the art of workflow composition</li> <li>Examples - See real-world examples and patterns</li> </ul>"},{"location":"#getting-help","title":"Getting Help","text":"<ul> <li>GitHub Issues: Report bugs or request features</li> <li>Discussions: Ask questions and share ideas</li> </ul> <p>Ready to orchestrate your workflows? Let's start with writing your first plugin!</p>"},{"location":"examples/","title":"Examples","text":"<p>This guide provides comprehensive, real-world examples of using AroFlow for various scenarios. Each example includes complete plugin implementations and workflow definitions you can adapt for your own use cases.</p>"},{"location":"examples/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Basic Examples</li> <li>Data Processing</li> <li>Web Scraping Pipeline</li> <li>Machine Learning Workflow</li> <li>API Integration</li> <li>File Processing</li> <li>Notification Systems</li> <li>Database Operations</li> </ul>"},{"location":"examples/#basic-examples","title":"Basic Examples","text":""},{"location":"examples/#hello-world","title":"Hello World","text":"<p>The simplest possible AroFlow workflow:</p> <pre><code>import aroflow\n\nclass GreetingPlugin(aroflow.PluginMixin):\n    plugin_name = \"greet\"\n\n    def execute(self, name: str, greeting: str = \"Hello\") -&gt; str:\n        return f\"{greeting}, {name}!\"\n\n# Setup\nclient = aroflow.create(aroflow.BackendType.IN_MEMORY)\nclient.plugin(GreetingPlugin)\n\n# Workflow\nworkflow = {\n    \"steps\": [\n        {\n            \"id\": \"say_hello\",\n            \"kind\": \"operation\",\n            \"operation\": \"greet\",\n            \"parameters\": {\n                \"name\": \"AroFlow\",\n                \"greeting\": \"Welcome\"\n            }\n        }\n    ]\n}\n\n# Execute\nresult = client.run(workflow)\nprint(result.to_yaml())\n</code></pre>"},{"location":"examples/#sequential-steps","title":"Sequential Steps","text":"<p>Chain operations together:</p> <pre><code>import aroflow\n\nclass MathPlugin(aroflow.PluginMixin):\n    plugin_name = \"math\"\n\n    def execute(self, operation: str, a: float, b: float) -&gt; float:\n        if operation == \"add\":\n            return a + b\n        elif operation == \"multiply\":\n            return a * b\n        elif operation == \"divide\":\n            return a / b if b != 0 else 0\n        else:\n            raise ValueError(f\"Unknown operation: {operation}\")\n\nclass FormatterPlugin(aroflow.PluginMixin):\n    plugin_name = \"format\"\n\n    def execute(self, value: float, format_type: str = \"decimal\") -&gt; str:\n        if format_type == \"decimal\":\n            return f\"{value:.2f}\"\n        elif format_type == \"currency\":\n            return f\"${value:.2f}\"\n        elif format_type == \"percentage\":\n            return f\"{value:.1%}\"\n        else:\n            return str(value)\n\n# Setup\nclient = aroflow.create(aroflow.BackendType.IN_MEMORY)\nclient.plugin(MathPlugin)\nclient.plugin(FormatterPlugin)\n\n# Workflow: Calculate compound interest\nworkflow = {\n    \"steps\": [\n        {\n            \"id\": \"calculate_interest\",\n            \"kind\": \"operation\",\n            \"operation\": \"math\",\n            \"parameters\": {\n                \"operation\": \"multiply\",\n                \"a\": 1000.0,  # Principal\n                \"b\": 1.05     # Interest rate\n            }\n        },\n        {\n            \"id\": \"format_result\",\n            \"kind\": \"operation\",\n            \"operation\": \"format\",\n            \"parameters\": {\n                \"value\": \"${calculate_interest.result}\",\n                \"format_type\": \"currency\"\n            }\n        }\n    ]\n}\n\nresult = client.run(workflow)\nprint(f\"Investment result: {result.results[1].result}\")\n</code></pre>"},{"location":"examples/#data-processing","title":"Data Processing","text":""},{"location":"examples/#csv-data-analysis","title":"CSV Data Analysis","text":"<p>Complete pipeline for analyzing CSV data:</p> <pre><code>import aroflow\nimport pandas as pd\nimport json\nfrom io import StringIO\n\nclass CSVReaderPlugin(aroflow.PluginMixin):\n    plugin_name = \"read_csv\"\n\n    def execute(self, file_path: str, delimiter: str = \",\") -&gt; dict:\n        df = pd.read_csv(file_path, delimiter=delimiter)\n        return {\n            \"data\": df.to_dict('records'),\n            \"columns\": df.columns.tolist(),\n            \"shape\": df.shape,\n            \"dtypes\": df.dtypes.to_dict()\n        }\n\nclass DataFilterPlugin(aroflow.PluginMixin):\n    plugin_name = \"filter_data\"\n\n    def execute(self, data: list, filter_column: str, filter_value: str) -&gt; dict:\n        filtered_data = [\n            row for row in data \n            if str(row.get(filter_column, \"\")).lower() == filter_value.lower()\n        ]\n        return {\n            \"data\": filtered_data,\n            \"original_count\": len(data),\n            \"filtered_count\": len(filtered_data)\n        }\n\nclass StatisticsPlugin(aroflow.PluginMixin):\n    plugin_name = \"calculate_stats\"\n\n    def execute(self, data: list, numeric_columns: list) -&gt; dict:\n        if not data:\n            return {\"error\": \"No data to analyze\"}\n\n        stats = {}\n        for column in numeric_columns:\n            values = [\n                float(row[column]) for row in data \n                if column in row and row[column] is not None\n            ]\n\n            if values:\n                stats[column] = {\n                    \"count\": len(values),\n                    \"mean\": sum(values) / len(values),\n                    \"min\": min(values),\n                    \"max\": max(values),\n                    \"median\": sorted(values)[len(values) // 2]\n                }\n\n        return stats\n\nclass ReportGeneratorPlugin(aroflow.PluginMixin):\n    plugin_name = \"generate_report\"\n\n    def execute(self, statistics: dict, title: str = \"Data Analysis Report\") -&gt; str:\n        report = f\"# {title}\\n\\n\"\n\n        for column, stats in statistics.items():\n            report += f\"## {column.title()}\\n\"\n            report += f\"- Count: {stats['count']}\\n\"\n            report += f\"- Mean: {stats['mean']:.2f}\\n\"\n            report += f\"- Min: {stats['min']:.2f}\\n\"\n            report += f\"- Max: {stats['max']:.2f}\\n\"\n            report += f\"- Median: {stats['median']:.2f}\\n\\n\"\n\n        return report\n\n# Setup\nclient = aroflow.create(aroflow.BackendType.IN_MEMORY)\nclient.plugin(CSVReaderPlugin)\nclient.plugin(DataFilterPlugin)\nclient.plugin(StatisticsPlugin)\nclient.plugin(ReportGeneratorPlugin)\n\n# Workflow: Analyze sales data\ndata_analysis_workflow = {\n    \"steps\": [\n        {\n            \"id\": \"load_sales_data\",\n            \"kind\": \"operation\",\n            \"operation\": \"read_csv\",\n            \"parameters\": {\n                \"file_path\": \"/data/sales.csv\",\n                \"delimiter\": \",\"\n            }\n        },\n        {\n            \"id\": \"filter_current_year\",\n            \"kind\": \"operation\",\n            \"operation\": \"filter_data\",\n            \"parameters\": {\n                \"data\": \"${load_sales_data.result.data}\",\n                \"filter_column\": \"year\",\n                \"filter_value\": \"2024\"\n            }\n        },\n        {\n            \"id\": \"calculate_statistics\",\n            \"kind\": \"operation\",\n            \"operation\": \"calculate_stats\",\n            \"parameters\": {\n                \"data\": \"${filter_current_year.result.data}\",\n                \"numeric_columns\": [\"revenue\", \"quantity\", \"profit_margin\"]\n            }\n        },\n        {\n            \"id\": \"generate_report\",\n            \"kind\": \"operation\", \n            \"operation\": \"generate_report\",\n            \"parameters\": {\n                \"statistics\": \"${calculate_statistics.result}\",\n                \"title\": \"2024 Sales Analysis Report\"\n            }\n        }\n    ]\n}\n</code></pre>"},{"location":"examples/#batch-data-processing","title":"Batch Data Processing","text":"<p>Process large datasets in batches:</p> <pre><code>import aroflow\n\nclass BatchProcessorPlugin(aroflow.PluginMixin):\n    plugin_name = \"batch_processor\"\n\n    def execute(self, data: list, batch_size: int = 100) -&gt; dict:\n        batches = []\n        for i in range(0, len(data), batch_size):\n            batch = data[i:i + batch_size]\n            batches.append({\n                \"batch_id\": i // batch_size,\n                \"data\": batch,\n                \"size\": len(batch)\n            })\n\n        return {\n            \"batches\": batches,\n            \"total_batches\": len(batches),\n            \"total_records\": len(data)\n        }\n\nclass RecordEnricherPlugin(aroflow.PluginMixin):\n    plugin_name = \"enrich_record\"\n\n    def execute(self, record: dict, enrichment_source: str = \"api\") -&gt; dict:\n        # Simulate enrichment (in real implementation, call external API)\n        enriched_record = record.copy()\n        enriched_record.update({\n            \"enriched_at\": \"2024-01-15T10:00:00Z\",\n            \"enrichment_source\": enrichment_source,\n            \"confidence_score\": 0.95\n        })\n        return enriched_record\n\nclass BatchResultsAggregatorPlugin(aroflow.PluginMixin):\n    plugin_name = \"aggregate_results\"\n\n    def execute(self, batch_results: list) -&gt; dict:\n        all_records = []\n        total_processed = 0\n\n        for batch_result in batch_results:\n            if \"results\" in batch_result:\n                all_records.extend(batch_result[\"results\"])\n                total_processed += len(batch_result[\"results\"])\n\n        return {\n            \"all_records\": all_records,\n            \"total_processed\": total_processed,\n            \"batches_processed\": len(batch_results)\n        }\n\n# Setup\nclient = aroflow.create(aroflow.BackendType.IN_MEMORY)\nclient.plugin(BatchProcessorPlugin)\nclient.plugin(RecordEnricherPlugin)\nclient.plugin(BatchResultsAggregatorPlugin)\n\n# Workflow: Process large dataset in batches\nbatch_processing_workflow = {\n    \"steps\": [\n        {\n            \"id\": \"create_batches\",\n            \"kind\": \"operation\",\n            \"operation\": \"batch_processor\",\n            \"parameters\": {\n                \"data\": \"${input_data}\",  # Assume this comes from previous step\n                \"batch_size\": 50\n            }\n        },\n        {\n            \"id\": \"process_batches\",\n            \"kind\": \"map\",\n            \"mode\": \"parallel\",\n            \"iterator\": \"batch\",\n            \"inputs\": \"${create_batches.result.batches}\",\n            \"operation\": {\n                \"id\": \"process_single_batch\",\n                \"kind\": \"map\",\n                \"mode\": \"sequential\",  # Process records in batch sequentially\n                \"iterator\": \"record\",\n                \"inputs\": \"${batch.data}\",\n                \"operation\": {\n                    \"id\": \"enrich_single_record\",\n                    \"kind\": \"operation\",\n                    \"operation\": \"enrich_record\",\n                    \"parameters\": {\n                        \"record\": \"${record}\",\n                        \"enrichment_source\": \"external_api\"\n                    }\n                }\n            }\n        },\n        {\n            \"id\": \"aggregate_results\",\n            \"kind\": \"operation\",\n            \"operation\": \"aggregate_results\",\n            \"parameters\": {\n                \"batch_results\": \"${process_batches.results}\"\n            }\n        }\n    ]\n}\n</code></pre>"},{"location":"examples/#web-scraping-pipeline","title":"Web Scraping Pipeline","text":"<p>Complete web scraping and data extraction pipeline:</p> <pre><code>import aroflow\nimport requests\nfrom bs4 import BeautifulSoup\nimport time\nimport random\n\nclass WebScraperPlugin(aroflow.PluginMixin):\n    plugin_name = \"scrape_web\"\n\n    def execute(\n        self, \n        url: str, \n        selector: str, \n        delay: float = 1.0,\n        user_agent: str = \"AroFlow WebScraper 1.0\"\n    ) -&gt; dict:\n        headers = {\"User-Agent\": user_agent}\n\n        # Add random delay to be respectful\n        time.sleep(random.uniform(delay, delay * 2))\n\n        try:\n            response = requests.get(url, headers=headers, timeout=30)\n            response.raise_for_status()\n\n            soup = BeautifulSoup(response.content, 'html.parser')\n            elements = soup.select(selector)\n\n            extracted_data = []\n            for element in elements:\n                extracted_data.append({\n                    \"text\": element.get_text(strip=True),\n                    \"html\": str(element),\n                    \"attributes\": dict(element.attrs)\n                })\n\n            return {\n                \"url\": url,\n                \"status_code\": response.status_code,\n                \"data\": extracted_data,\n                \"count\": len(extracted_data),\n                \"scraped_at\": time.strftime(\"%Y-%m-%d %H:%M:%S\")\n            }\n\n        except Exception as e:\n            return {\n                \"url\": url,\n                \"error\": str(e),\n                \"scraped_at\": time.strftime(\"%Y-%m-%d %H:%M:%S\")\n            }\n\nclass DataCleanerPlugin(aroflow.PluginMixin):\n    plugin_name = \"clean_scraped_data\"\n\n    def execute(self, scraped_results: list, remove_duplicates: bool = True) -&gt; dict:\n        all_data = []\n\n        for result in scraped_results:\n            if \"data\" in result:\n                for item in result[\"data\"]:\n                    cleaned_item = {\n                        \"source_url\": result[\"url\"],\n                        \"text\": item[\"text\"].strip(),\n                        \"scraped_at\": result[\"scraped_at\"]\n                    }\n                    all_data.append(cleaned_item)\n\n        if remove_duplicates:\n            seen = set()\n            unique_data = []\n            for item in all_data:\n                key = (item[\"text\"], item[\"source_url\"])\n                if key not in seen:\n                    seen.add(key)\n                    unique_data.append(item)\n            all_data = unique_data\n\n        return {\n            \"cleaned_data\": all_data,\n            \"total_items\": len(all_data),\n            \"sources_scraped\": len(set(item[\"source_url\"] for item in all_data))\n        }\n\nclass DataExporterPlugin(aroflow.PluginMixin):\n    plugin_name = \"export_data\"\n\n    def execute(self, data: list, format: str = \"json\", output_path: str = None) -&gt; dict:\n        if format == \"json\":\n            import json\n            output = json.dumps(data, indent=2)\n        elif format == \"csv\":\n            import csv\n            from io import StringIO\n            output_buffer = StringIO()\n            if data:\n                writer = csv.DictWriter(output_buffer, fieldnames=data[0].keys())\n                writer.writeheader()\n                writer.writerows(data)\n            output = output_buffer.getvalue()\n        else:\n            raise ValueError(f\"Unsupported format: {format}\")\n\n        if output_path:\n            with open(output_path, 'w') as f:\n                f.write(output)\n            return {\"message\": f\"Data exported to {output_path}\", \"format\": format}\n        else:\n            return {\"data\": output, \"format\": format}\n\n# Setup\nclient = aroflow.create(aroflow.BackendType.IN_MEMORY)\nclient.plugin(WebScraperPlugin)\nclient.plugin(DataCleanerPlugin)\nclient.plugin(DataExporterPlugin)\n\n# Workflow: Scrape news headlines\nnews_scraping_workflow = {\n    \"steps\": [\n        {\n            \"id\": \"scrape_news_sites\",\n            \"kind\": \"map\",\n            \"mode\": \"sequential\",  # Be respectful to websites\n            \"iterator\": \"site_config\",\n            \"inputs\": [\n                {\n                    \"url\": \"https://news.ycombinator.com\",\n                    \"selector\": \".storylink\",\n                    \"name\": \"Hacker News\"\n                },\n                {\n                    \"url\": \"https://www.reddit.com/r/technology\",\n                    \"selector\": \".entry-title a\", \n                    \"name\": \"Reddit Technology\"\n                }\n            ],\n            \"operation\": {\n                \"id\": \"scrape_single_site\",\n                \"kind\": \"operation\",\n                \"operation\": \"scrape_web\",\n                \"parameters\": {\n                    \"url\": \"${site_config.url}\",\n                    \"selector\": \"${site_config.selector}\",\n                    \"delay\": 2.0\n                }\n            }\n        },\n        {\n            \"id\": \"clean_and_deduplicate\",\n            \"kind\": \"operation\",\n            \"operation\": \"clean_scraped_data\",\n            \"parameters\": {\n                \"scraped_results\": \"${scrape_news_sites.results}\",\n                \"remove_duplicates\": True\n            }\n        },\n        {\n            \"id\": \"export_results\",\n            \"kind\": \"parallel\",\n            \"operations\": [\n                {\n                    \"id\": \"export_json\",\n                    \"kind\": \"operation\",\n                    \"operation\": \"export_data\",\n                    \"parameters\": {\n                        \"data\": \"${clean_and_deduplicate.result.cleaned_data}\",\n                        \"format\": \"json\",\n                        \"output_path\": \"/output/news_headlines.json\"\n                    }\n                },\n                {\n                    \"id\": \"export_csv\",\n                    \"kind\": \"operation\",\n                    \"operation\": \"export_data\",\n                    \"parameters\": {\n                        \"data\": \"${clean_and_deduplicate.result.cleaned_data}\",\n                        \"format\": \"csv\",\n                        \"output_path\": \"/output/news_headlines.csv\"\n                    }\n                }\n            ]\n        }\n    ]\n}\n</code></pre>"},{"location":"examples/#machine-learning-workflow","title":"Machine Learning Workflow","text":"<p>End-to-end ML pipeline with data preprocessing, model training, and evaluation:</p> <pre><code>import aroflow\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, classification_report\nimport joblib\n\nclass DataLoaderPlugin(aroflow.PluginMixin):\n    plugin_name = \"load_ml_data\"\n\n    def execute(self, file_path: str, target_column: str) -&gt; dict:\n        df = pd.read_csv(file_path)\n\n        # Separate features and target\n        X = df.drop(columns=[target_column])\n        y = df[target_column]\n\n        return {\n            \"features\": X.to_dict('records'),\n            \"target\": y.tolist(),\n            \"feature_names\": X.columns.tolist(),\n            \"target_name\": target_column,\n            \"shape\": df.shape\n        }\n\nclass DataPreprocessorPlugin(aroflow.PluginMixin):\n    plugin_name = \"preprocess_data\"\n\n    def execute(self, features: list, target: list, feature_names: list) -&gt; dict:\n        df = pd.DataFrame(features)\n\n        # Handle missing values\n        df = df.fillna(df.mean() if df.select_dtypes(include=[np.number]).shape[1] &gt; 0 else df.mode().iloc[0])\n\n        # Encode categorical variables\n        categorical_columns = df.select_dtypes(include=['object']).columns\n        label_encoders = {}\n\n        for col in categorical_columns:\n            le = LabelEncoder()\n            df[col] = le.fit_transform(df[col].astype(str))\n            label_encoders[col] = le\n\n        # Scale numerical features\n        scaler = StandardScaler()\n        X_scaled = scaler.fit_transform(df)\n\n        return {\n            \"features\": X_scaled.tolist(),\n            \"target\": target,\n            \"feature_names\": feature_names,\n            \"scaler\": scaler,\n            \"label_encoders\": label_encoders,\n            \"preprocessed_shape\": X_scaled.shape\n        }\n\nclass TrainTestSplitPlugin(aroflow.PluginMixin):\n    plugin_name = \"train_test_split\"\n\n    def execute(\n        self, \n        features: list, \n        target: list, \n        test_size: float = 0.2, \n        random_state: int = 42\n    ) -&gt; dict:\n        X_train, X_test, y_train, y_test = train_test_split(\n            features, target, test_size=test_size, random_state=random_state\n        )\n\n        return {\n            \"X_train\": X_train,\n            \"X_test\": X_test,\n            \"y_train\": y_train,\n            \"y_test\": y_test,\n            \"train_size\": len(X_train),\n            \"test_size\": len(X_test)\n        }\n\nclass ModelTrainerPlugin(aroflow.PluginMixin):\n    plugin_name = \"train_model\"\n\n    def execute(\n        self, \n        X_train: list, \n        y_train: list, \n        model_type: str = \"random_forest\",\n        **model_params\n    ) -&gt; dict:\n        if model_type == \"random_forest\":\n            model = RandomForestClassifier(**model_params)\n        elif model_type == \"logistic_regression\":\n            model = LogisticRegression(**model_params)\n        else:\n            raise ValueError(f\"Unsupported model type: {model_type}\")\n\n        # Train the model\n        model.fit(X_train, y_train)\n\n        return {\n            \"model\": model,\n            \"model_type\": model_type,\n            \"model_params\": model_params,\n            \"feature_importances\": (\n                model.feature_importances_.tolist() \n                if hasattr(model, 'feature_importances_') else None\n            )\n        }\n\nclass ModelEvaluatorPlugin(aroflow.PluginMixin):\n    plugin_name = \"evaluate_model\"\n\n    def execute(self, model, X_test: list, y_test: list, model_type: str) -&gt; dict:\n        # Make predictions\n        y_pred = model.predict(X_test)\n\n        # Calculate metrics\n        accuracy = accuracy_score(y_test, y_pred)\n        report = classification_report(y_test, y_pred, output_dict=True)\n\n        return {\n            \"accuracy\": accuracy,\n            \"classification_report\": report,\n            \"predictions\": y_pred.tolist(),\n            \"model_type\": model_type,\n            \"test_samples\": len(y_test)\n        }\n\nclass ModelSaverPlugin(aroflow.PluginMixin):\n    plugin_name = \"save_model\"\n\n    def execute(\n        self, \n        model, \n        scaler, \n        label_encoders: dict, \n        model_path: str,\n        metadata: dict = None\n    ) -&gt; dict:\n        # Save model artifacts\n        joblib.dump({\n            \"model\": model,\n            \"scaler\": scaler,\n            \"label_encoders\": label_encoders,\n            \"metadata\": metadata or {}\n        }, model_path)\n\n        return {\n            \"message\": f\"Model saved to {model_path}\",\n            \"model_path\": model_path,\n            \"artifacts\": [\"model\", \"scaler\", \"label_encoders\", \"metadata\"]\n        }\n\n# Setup\nclient = aroflow.create(aroflow.BackendType.IN_MEMORY)\nclient.plugin(DataLoaderPlugin)\nclient.plugin(DataPreprocessorPlugin)\nclient.plugin(TrainTestSplitPlugin)\nclient.plugin(ModelTrainerPlugin)\nclient.plugin(ModelEvaluatorPlugin)\nclient.plugin(ModelSaverPlugin)\n\n# ML Workflow\nml_pipeline = {\n    \"steps\": [\n        {\n            \"id\": \"load_data\",\n            \"kind\": \"operation\",\n            \"operation\": \"load_ml_data\",\n            \"parameters\": {\n                \"file_path\": \"/data/iris.csv\",\n                \"target_column\": \"species\"\n            }\n        },\n        {\n            \"id\": \"preprocess_data\",\n            \"kind\": \"operation\",\n            \"operation\": \"preprocess_data\",\n            \"parameters\": {\n                \"features\": \"${load_data.result.features}\",\n                \"target\": \"${load_data.result.target}\",\n                \"feature_names\": \"${load_data.result.feature_names}\"\n            }\n        },\n        {\n            \"id\": \"split_data\",\n            \"kind\": \"operation\",\n            \"operation\": \"train_test_split\",\n            \"parameters\": {\n                \"features\": \"${preprocess_data.result.features}\",\n                \"target\": \"${preprocess_data.result.target}\",\n                \"test_size\": 0.2,\n                \"random_state\": 42\n            }\n        },\n        {\n            \"id\": \"train_models\",\n            \"kind\": \"map\",\n            \"mode\": \"parallel\",\n            \"iterator\": \"model_config\",\n            \"inputs\": [\n                {\n                    \"model_type\": \"random_forest\",\n                    \"n_estimators\": 100,\n                    \"max_depth\": 10,\n                    \"random_state\": 42\n                },\n                {\n                    \"model_type\": \"logistic_regression\",\n                    \"max_iter\": 1000,\n                    \"random_state\": 42\n                }\n            ],\n            \"operation\": {\n                \"id\": \"train_single_model\",\n                \"kind\": \"operation\",\n                \"operation\": \"train_model\",\n                \"parameters\": {\n                    \"X_train\": \"${split_data.result.X_train}\",\n                    \"y_train\": \"${split_data.result.y_train}\",\n                    \"model_type\": \"${model_config.model_type}\",\n                    \"n_estimators\": \"${model_config.n_estimators}\",\n                    \"max_depth\": \"${model_config.max_depth}\",\n                    \"max_iter\": \"${model_config.max_iter}\",\n                    \"random_state\": \"${model_config.random_state}\"\n                }\n            }\n        },\n        {\n            \"id\": \"evaluate_models\",\n            \"kind\": \"map\",\n            \"mode\": \"parallel\",\n            \"iterator\": \"trained_model\",\n            \"inputs\": \"${train_models.results}\",\n            \"operation\": {\n                \"id\": \"evaluate_single_model\",\n                \"kind\": \"operation\",\n                \"operation\": \"evaluate_model\",\n                \"parameters\": {\n                    \"model\": \"${trained_model.result.model}\",\n                    \"X_test\": \"${split_data.result.X_test}\",\n                    \"y_test\": \"${split_data.result.y_test}\",\n                    \"model_type\": \"${trained_model.result.model_type}\"\n                }\n            }\n        },\n        {\n            \"id\": \"save_best_model\",\n            \"kind\": \"operation\",\n            \"operation\": \"save_model\",\n            \"parameters\": {\n                \"model\": \"${train_models.results[0].result.model}\",  # Assuming first model is best\n                \"scaler\": \"${preprocess_data.result.scaler}\",\n                \"label_encoders\": \"${preprocess_data.result.label_encoders}\",\n                \"model_path\": \"/models/best_model.pkl\",\n                \"metadata\": {\n                    \"accuracy\": \"${evaluate_models.results[0].result.accuracy}\",\n                    \"model_type\": \"${evaluate_models.results[0].result.model_type}\",\n                    \"training_date\": \"2024-01-15\"\n                }\n            }\n        }\n    ]\n}\n</code></pre>"},{"location":"examples/#api-integration","title":"API Integration","text":"<p>Integrate with external APIs and handle responses:</p> <pre><code>import aroflow\nimport requests\nimport time\n\nclass APIClientPlugin(aroflow.PluginMixin):\n    plugin_name = \"api_client\"\n\n    def execute(\n        self,\n        url: str,\n        method: str = \"GET\",\n        headers: dict = None,\n        data: dict = None,\n        params: dict = None,\n        timeout: int = 30,\n        retries: int = 3\n    ) -&gt; dict:\n        headers = headers or {}\n\n        for attempt in range(retries + 1):\n            try:\n                response = requests.request(\n                    method=method.upper(),\n                    url=url,\n                    headers=headers,\n                    json=data if method.upper() in [\"POST\", \"PUT\", \"PATCH\"] else None,\n                    params=params,\n                    timeout=timeout\n                )\n\n                result = {\n                    \"status_code\": response.status_code,\n                    \"headers\": dict(response.headers),\n                    \"url\": response.url,\n                    \"elapsed_seconds\": response.elapsed.total_seconds()\n                }\n\n                # Try to parse JSON response\n                try:\n                    result[\"data\"] = response.json()\n                except:\n                    result[\"data\"] = response.text\n\n                # Check if request was successful\n                if response.status_code &lt; 400:\n                    result[\"success\"] = True\n                    return result\n                else:\n                    result[\"success\"] = False\n                    result[\"error\"] = f\"HTTP {response.status_code}: {response.reason}\"\n\n                    if attempt == retries:\n                        return result\n\n            except requests.exceptions.RequestException as e:\n                if attempt == retries:\n                    return {\n                        \"success\": False,\n                        \"error\": str(e),\n                        \"url\": url,\n                        \"attempt\": attempt + 1\n                    }\n                time.sleep(2 ** attempt)  # Exponential backoff\n\nclass DataTransformerPlugin(aroflow.PluginMixin):\n    plugin_name = \"transform_api_data\"\n\n    def execute(self, api_responses: list, transform_rules: dict) -&gt; dict:\n        transformed_data = []\n\n        for response in api_responses:\n            if response.get(\"success\") and \"data\" in response:\n                data = response[\"data\"]\n\n                # Apply transformation rules\n                if isinstance(data, list):\n                    for item in data:\n                        transformed_item = self._transform_item(item, transform_rules)\n                        transformed_data.append(transformed_item)\n                elif isinstance(data, dict):\n                    transformed_item = self._transform_item(data, transform_rules)\n                    transformed_data.append(transformed_item)\n\n        return {\n            \"transformed_data\": transformed_data,\n            \"total_items\": len(transformed_data),\n            \"transformation_rules\": transform_rules\n        }\n\n    def _transform_item(self, item: dict, rules: dict) -&gt; dict:\n        transformed = {}\n\n        for new_key, rule in rules.items():\n            if isinstance(rule, str):\n                # Simple field mapping\n                transformed[new_key] = item.get(rule)\n            elif isinstance(rule, dict):\n                # Complex transformation\n                if \"source_field\" in rule:\n                    value = item.get(rule[\"source_field\"])\n\n                    # Apply transformation\n                    if \"transform\" in rule:\n                        if rule[\"transform\"] == \"upper\":\n                            value = str(value).upper() if value else value\n                        elif rule[\"transform\"] == \"lower\":\n                            value = str(value).lower() if value else value\n                        elif rule[\"transform\"] == \"int\":\n                            value = int(value) if value else 0\n\n                    transformed[new_key] = value\n\n        return transformed\n\nclass DataValidatorPlugin(aroflow.PluginMixin):\n    plugin_name = \"validate_data\"\n\n    def execute(self, data: list, validation_rules: dict) -&gt; dict:\n        valid_records = []\n        invalid_records = []\n\n        for record in data:\n            validation_errors = []\n\n            # Check required fields\n            for field in validation_rules.get(\"required_fields\", []):\n                if field not in record or record[field] is None:\n                    validation_errors.append(f\"Missing required field: {field}\")\n\n            # Check field types\n            for field, expected_type in validation_rules.get(\"field_types\", {}).items():\n                if field in record and record[field] is not None:\n                    if expected_type == \"string\" and not isinstance(record[field], str):\n                        validation_errors.append(f\"Field {field} must be string\")\n                    elif expected_type == \"int\" and not isinstance(record[field], int):\n                        validation_errors.append(f\"Field {field} must be integer\")\n\n            # Add validation results\n            record_with_validation = record.copy()\n            record_with_validation[\"_validation_errors\"] = validation_errors\n\n            if validation_errors:\n                invalid_records.append(record_with_validation)\n            else:\n                valid_records.append(record)\n\n        return {\n            \"valid_records\": valid_records,\n            \"invalid_records\": invalid_records,\n            \"total_records\": len(data),\n            \"valid_count\": len(valid_records),\n            \"invalid_count\": len(invalid_records),\n            \"validation_rate\": len(valid_records) / len(data) if data else 0\n        }\n\n# Setup\nclient = aroflow.create(aroflow.BackendType.IN_MEMORY)\nclient.plugin(APIClientPlugin)\nclient.plugin(DataTransformerPlugin)\nclient.plugin(DataValidatorPlugin)\n\n# API Integration Workflow\napi_workflow = {\n    \"steps\": [\n        {\n            \"id\": \"fetch_user_data\",\n            \"kind\": \"map\",\n            \"mode\": \"parallel\",\n            \"iterator\": \"user_id\",\n            \"inputs\": [\"1\", \"2\", \"3\", \"4\", \"5\"],\n            \"operation\": {\n                \"id\": \"get_user\",\n                \"kind\": \"operation\",\n                \"operation\": \"api_client\",\n                \"parameters\": {\n                    \"url\": \"https://jsonplaceholder.typicode.com/users/${user_id}\",\n                    \"method\": \"GET\",\n                    \"timeout\": 10,\n                    \"retries\": 2\n                }\n            }\n        },\n        {\n            \"id\": \"fetch_user_posts\",\n            \"kind\": \"map\",\n            \"mode\": \"parallel\",\n            \"iterator\": \"user_id\",\n            \"inputs\": [\"1\", \"2\", \"3\", \"4\", \"5\"],\n            \"operation\": {\n                \"id\": \"get_posts\",\n                \"kind\": \"operation\",\n                \"operation\": \"api_client\",\n                \"parameters\": {\n                    \"url\": \"https://jsonplaceholder.typicode.com/posts\",\n                    \"method\": \"GET\",\n                    \"params\": {\"userId\": \"${user_id}\"},\n                    \"timeout\": 10\n                }\n            }\n        },\n        {\n            \"id\": \"transform_user_data\",\n            \"kind\": \"operation\",\n            \"operation\": \"transform_api_data\",\n            \"parameters\": {\n                \"api_responses\": \"${fetch_user_data.results}\",\n                \"transform_rules\": {\n                    \"user_id\": \"id\",\n                    \"username\": \"username\",\n                    \"email\": \"email\",\n                    \"full_name\": \"name\",\n                    \"city\": {\n                        \"source_field\": \"address.city\",\n                        \"transform\": \"title\"\n                    }\n                }\n            }\n        },\n        {\n            \"id\": \"validate_transformed_data\",\n            \"kind\": \"operation\",\n            \"operation\": \"validate_data\",\n            \"parameters\": {\n                \"data\": \"${transform_user_data.result.transformed_data}\",\n                \"validation_rules\": {\n                    \"required_fields\": [\"user_id\", \"username\", \"email\"],\n                    \"field_types\": {\n                        \"user_id\": \"int\",\n                        \"username\": \"string\",\n                        \"email\": \"string\"\n                    }\n                }\n            }\n        }\n    ]\n}\n</code></pre> <p>This comprehensive examples guide demonstrates the power and flexibility of AroFlow across different domains. Each example is complete and runnable, showing best practices for plugin development and workflow composition.</p>"},{"location":"examples/#file-processing","title":"File Processing","text":"<p>Process files and directories with AroFlow:</p> <pre><code>import aroflow\nimport os\nimport shutil\nfrom pathlib import Path\n\nclass FileProcessorPlugin(aroflow.PluginMixin):\n    plugin_name = \"process_file\"\n\n    def execute(self, file_path: str, operation: str, **kwargs) -&gt; dict:\n        path = Path(file_path)\n\n        if operation == \"copy\":\n            dest = kwargs.get(\"destination\")\n            shutil.copy2(file_path, dest)\n            return {\"message\": f\"Copied {file_path} to {dest}\"}\n\n        elif operation == \"move\":\n            dest = kwargs.get(\"destination\")\n            shutil.move(file_path, dest)\n            return {\"message\": f\"Moved {file_path} to {dest}\"}\n\n        elif operation == \"delete\":\n            os.remove(file_path)\n            return {\"message\": f\"Deleted {file_path}\"}\n\n        elif operation == \"info\":\n            stat = path.stat()\n            return {\n                \"name\": path.name,\n                \"size\": stat.st_size,\n                \"modified\": stat.st_mtime,\n                \"exists\": path.exists()\n            }\n</code></pre>"},{"location":"examples/#notification-systems","title":"Notification Systems","text":"<p>Send notifications through various channels:</p> <pre><code>import aroflow\nimport smtplib\nfrom email.mime.text import MIMEText\n\nclass EmailNotificationPlugin(aroflow.PluginMixin):\n    plugin_name = \"send_email\"\n\n    def execute(self, to: str, subject: str, body: str, smtp_server: str = \"localhost\") -&gt; dict:\n        msg = MIMEText(body)\n        msg['Subject'] = subject\n        msg['To'] = to\n        msg['From'] = \"noreply@aroflow.dev\"\n\n        try:\n            with smtplib.SMTP(smtp_server) as server:\n                server.send_message(msg)\n            return {\"status\": \"sent\", \"to\": to, \"subject\": subject}\n        except Exception as e:\n            return {\"status\": \"failed\", \"error\": str(e)}\n\nclass SlackNotificationPlugin(aroflow.PluginMixin):\n    plugin_name = \"send_slack\"\n\n    def execute(self, channel: str, message: str, webhook_url: str) -&gt; dict:\n        import requests\n\n        payload = {\n            \"channel\": channel,\n            \"text\": message,\n            \"username\": \"AroFlow\"\n        }\n\n        response = requests.post(webhook_url, json=payload)\n        return {\n            \"status\": \"sent\" if response.ok else \"failed\",\n            \"channel\": channel,\n            \"status_code\": response.status_code\n        }\n</code></pre>"},{"location":"examples/#database-operations","title":"Database Operations","text":"<p>Interact with databases using AroFlow:</p> <pre><code>import aroflow\nimport sqlite3\n\nclass DatabasePlugin(aroflow.PluginMixin):\n    plugin_name = \"database\"\n\n    def execute(self, operation: str, query: str, db_path: str = \"data.db\", params: list = None) -&gt; dict:\n        conn = sqlite3.connect(db_path)\n        cursor = conn.cursor()\n\n        try:\n            if operation == \"select\":\n                cursor.execute(query, params or [])\n                results = cursor.fetchall()\n                columns = [desc[0] for desc in cursor.description]\n                return {\n                    \"results\": [dict(zip(columns, row)) for row in results],\n                    \"count\": len(results)\n                }\n            elif operation in [\"insert\", \"update\", \"delete\"]:\n                cursor.execute(query, params or [])\n                conn.commit()\n                return {\n                    \"rows_affected\": cursor.rowcount,\n                    \"operation\": operation\n                }\n        finally:\n            conn.close()\n</code></pre>"},{"location":"examples/#running-the-examples","title":"Running the Examples","text":"<p>To run any of these examples:</p> <ol> <li>Install AroFlow: <code>pip install aroflow</code></li> <li>Copy the plugin code into your Python file</li> <li>Set up the client and register plugins</li> <li>Define your workflow </li> <li>Execute with <code>client.run(workflow)</code></li> </ol>"},{"location":"examples/#customization-tips","title":"Customization Tips","text":"<ul> <li>Modify plugin parameters to fit your specific use cases</li> <li>Combine workflows by using outputs from one as inputs to another</li> <li>Add error handling with retries and timeouts</li> <li>Scale with parallel processing using map and parallel steps</li> <li>Extend plugins with additional functionality as needed</li> </ul> <p>Ready to build your own workflows? Start with the plugins guide or workflow documentation!</p>"},{"location":"plugins/","title":"Writing Plugins","text":"<p>Plugins are the heart of AroFlow. They encapsulate your business logic and make it available to workflows. Plugins are Python classes or instances that extend <code>aroflow.PluginMixin</code>. You can register either plugin classes or plugin instances with the client, depending on whether your plugin requires configuration or state.</p>"},{"location":"plugins/#plugin-basics","title":"Plugin Basics","text":""},{"location":"plugins/#what-is-a-plugin","title":"What is a Plugin?","text":"<p>A plugin is a Python class or instance that extends <code>aroflow.PluginMixin</code> and implements an <code>execute</code> method. Each plugin declares a unique <code>plugin_name</code> that workflows use to reference it within a client.</p> <pre><code>import aroflow\n\nclass MyPlugin(aroflow.PluginMixin):\n    plugin_name = \"my_plugin\"\n\n    def execute(self, param1: str, param2: int = 10) -&gt; str:\n        return f\"Processed {param1} with value {param2}\"\n</code></pre>"},{"location":"plugins/#plugin-requirements","title":"Plugin Requirements","text":"<p>Every plugin must:</p> <ol> <li>Inherit from <code>aroflow.PluginMixin</code></li> <li>Define a unique <code>plugin_name</code> (string) within the client registry</li> <li>Implement an <code>execute</code> method</li> </ol> <p>The <code>plugin_name</code> must be unique among all plugins registered with the same client, but does not need to be globally unique across different clients.</p> <p>The <code>execute</code> method can have any signature you need. AroFlow maps workflow parameters to the <code>execute</code> method arguments by name and performs type coercion for primitive types (e.g., <code>str</code>, <code>int</code>, <code>float</code>, <code>bool</code>). It does not automatically handle arbitrary signatures or complex conversions.</p>"},{"location":"plugins/#plugin-examples","title":"Plugin Examples","text":""},{"location":"plugins/#simple-data-processor","title":"Simple Data Processor","text":"<pre><code>import aroflow\n\nclass DataProcessorPlugin(aroflow.PluginMixin):\n    plugin_name = \"process_data\"\n\n    def execute(self, data: str, transform: str = \"upper\") -&gt; str:\n        \"\"\"Process text data with various transformations.\"\"\"\n        if transform == \"upper\":\n            return data.upper()\n        elif transform == \"lower\":\n            return data.lower()\n        elif transform == \"reverse\":\n            return data[::-1]\n        else:\n            return data\n</code></pre> <p>Usage in workflow:</p> <pre><code>{\n    \"id\": \"step1\",\n    \"kind\": \"operation\",\n    \"operation\": \"process_data\",\n    \"parameters\": {\n        \"data\": \"Hello World\",\n        \"transform\": \"upper\"\n    }\n}\n</code></pre>"},{"location":"plugins/#file-operation-plugin","title":"File Operation Plugin","text":"<pre><code>import aroflow\nimport json\nfrom pathlib import Path\n\nclass FileOperationPlugin(aroflow.PluginMixin):\n    plugin_name = \"file_ops\"\n\n    def execute(self, operation: str, file_path: str, content: str = None) -&gt; dict:\n        \"\"\"Perform various file operations.\"\"\"\n        path = Path(file_path)\n\n        if operation == \"read\":\n            if not path.exists():\n                raise FileNotFoundError(f\"File {file_path} not found\")\n            return {\"content\": path.read_text(), \"size\": path.stat().st_size}\n\n        elif operation == \"write\":\n            if content is None:\n                raise ValueError(\"Content required for write operation\")\n            path.write_text(content)\n            return {\"message\": f\"Written to {file_path}\", \"size\": len(content)}\n\n        elif operation == \"delete\":\n            if path.exists():\n                path.unlink()\n                return {\"message\": f\"Deleted {file_path}\"}\n            else:\n                return {\"message\": f\"File {file_path} did not exist\"}\n\n        else:\n            raise ValueError(f\"Unknown operation: {operation}\")\n</code></pre>"},{"location":"plugins/#http-request-plugin","title":"HTTP Request Plugin","text":"<pre><code>import aroflow\nimport requests\nfrom typing import Optional, Dict, Any\n\nclass HttpRequestPlugin(aroflow.PluginMixin):\n    plugin_name = \"http_request\"\n\n    def execute(\n        self, \n        url: str, \n        method: str = \"GET\", \n        headers: Optional[Dict[str, str]] = None,\n        data: Optional[Dict[str, Any]] = None,\n        timeout: int = 30\n    ) -&gt; dict:\n        \"\"\"Make HTTP requests.\"\"\"\n\n        response = requests.request(\n            method=method.upper(),\n            url=url,\n            headers=headers or {},\n            json=data if method.upper() != \"GET\" else None,\n            timeout=timeout\n        )\n\n        # Explicitly parse JSON only if content-type header indicates JSON\n        content_type = response.headers.get(\"content-type\", \"\")\n        try:\n            json_body = response.json() if \"application/json\" in content_type else None\n        except ValueError:\n            json_body = None\n\n        return {\n            \"status_code\": response.status_code,\n            \"headers\": dict(response.headers),\n            \"body\": response.text,\n            \"json\": json_body\n        }\n</code></pre>"},{"location":"plugins/#parameter-handling","title":"Parameter Handling","text":""},{"location":"plugins/#type-annotations","title":"Type Annotations","text":"<p>AroFlow uses type annotations to convert parameters from workflow definitions where possible:</p> <pre><code>class TypedPlugin(aroflow.PluginMixin):\n    plugin_name = \"typed_plugin\"\n\n    def execute(\n        self,\n        name: str,           # Will be converted to string\n        age: int,            # Will be converted to integer\n        height: float,       # Will be converted to float\n        active: bool,        # Will be converted to boolean\n        tags: list,          # Will remain as list\n        metadata: dict       # Will remain as dict\n    ) -&gt; dict:\n        return {\n            \"name\": name,\n            \"age\": age,\n            \"height\": height,\n            \"active\": active,\n            \"tags\": tags,\n            \"metadata\": metadata\n        }\n</code></pre>"},{"location":"plugins/#optional-parameters","title":"Optional Parameters","text":"<p>Use default values for optional parameters:</p> <pre><code>class ConfigurablePlugin(aroflow.PluginMixin):\n    plugin_name = \"configurable\"\n\n    def execute(\n        self,\n        required_param: str,\n        optional_param: str = \"default_value\",\n        retry_count: int = 3,\n        debug: bool = False\n    ) -&gt; str:\n        result = f\"Processing {required_param}\"\n        if debug:\n            result += f\" (retries: {retry_count}, optional: {optional_param})\"\n        return result\n</code></pre>"},{"location":"plugins/#variable-arguments","title":"Variable Arguments","text":"<p>Plugins can accept variable arguments and keyword arguments:</p> <pre><code>class FlexiblePlugin(aroflow.PluginMixin):\n    plugin_name = \"flexible\"\n\n    def execute(self, operation: str, *args, **kwargs) -&gt; dict:\n        return {\n            \"operation\": operation,\n            \"args\": args,\n            \"kwargs\": kwargs\n        }\n</code></pre>"},{"location":"plugins/#error-handling","title":"Error Handling","text":""},{"location":"plugins/#raising-exceptions","title":"Raising Exceptions","text":"<p>Plugins should raise meaningful exceptions when errors occur:</p> <pre><code>class ValidatingPlugin(aroflow.PluginMixin):\n    plugin_name = \"validator\"\n\n    def execute(self, email: str, age: int) -&gt; dict:\n        # Validate email\n        if \"@\" not in email:\n            raise ValueError(f\"Invalid email format: {email}\")\n\n        # Validate age\n        if age &lt; 0 or age &gt; 150:\n            raise ValueError(f\"Invalid age: {age}\")\n\n        return {\"email\": email, \"age\": age, \"valid\": True}\n</code></pre>"},{"location":"plugins/#custom-exception-classes","title":"Custom Exception Classes","text":"<p>For better error handling, create custom exception classes:</p> <pre><code>class ProcessingError(Exception):\n    \"\"\"Custom exception for processing errors.\"\"\"\n    pass\n\nclass DataProcessorPlugin(aroflow.PluginMixin):\n    plugin_name = \"data_processor\"\n\n    def execute(self, data: str, format: str) -&gt; dict:\n        try:\n            if format == \"json\":\n                import json\n                result = json.loads(data)\n            elif format == \"csv\":\n                import csv\n                result = list(csv.reader([data]))\n            else:\n                raise ProcessingError(f\"Unsupported format: {format}\")\n\n            return {\"parsed\": result, \"format\": format}\n\n        except json.JSONDecodeError as e:\n            raise ProcessingError(f\"Invalid JSON: {e}\")\n        except Exception as e:\n            raise ProcessingError(f\"Processing failed: {e}\")\n</code></pre>"},{"location":"plugins/#advanced-plugin-patterns","title":"Advanced Plugin Patterns","text":""},{"location":"plugins/#stateful-plugins","title":"Stateful Plugins","text":"<p>Note: Stateful plugins are not officially supported in AroFlow. Plugins should generally be stateless, unless they explicitly manage configuration or interact with external state (such as databases or files). If you need to maintain state, consider handling it through external resources or configuration passed to the plugin at initialization.</p>"},{"location":"plugins/#plugin-registration","title":"Plugin Registration","text":""},{"location":"plugins/#registering-plugins","title":"Registering Plugins","text":"<p>Register plugins with the client before running workflows:</p> <pre><code>import aroflow\n\n# Create client\nclient = aroflow.create(aroflow.BackendType.IN_MEMORY)\n\n# Register multiple plugin classes\nclient.plugin(DataProcessorPlugin)\nclient.plugin(FileOperationPlugin)\nclient.plugin(HttpRequestPlugin)\n\n# Register plugin instances if they require configuration or state\ndb_plugin = DatabasePlugin(\"connection_string\")\nclient.plugin(db_plugin)\n</code></pre>"},{"location":"plugins/#plugin-discovery","title":"Plugin Discovery","text":"<p>Plugins are automatically discovered when registered. The <code>plugin_name</code> must be unique across all plugins registered with the same client.</p>"},{"location":"plugins/#best-practices","title":"Best Practices","text":""},{"location":"plugins/#1-clear-naming","title":"1. Clear Naming","text":"<p>Use descriptive plugin names that reflect their purpose:</p> <pre><code># Good\nplugin_name = \"send_email\"\nplugin_name = \"process_csv\"\nplugin_name = \"validate_data\"\n\n# Avoid\nplugin_name = \"plugin1\"\nplugin_name = \"helper\"\nplugin_name = \"util\"\n</code></pre>"},{"location":"plugins/#2-type-annotations","title":"2. Type Annotations","text":"<p>Always use type annotations for better parameter binding:</p> <pre><code>def execute(self, data: str, count: int, active: bool = True) -&gt; dict:\n    # Type annotations help AroFlow convert parameters correctly\n    pass\n</code></pre>"},{"location":"plugins/#3-meaningful-return-values","title":"3. Meaningful Return Values","text":"<p>Return structured data that other steps can use:</p> <pre><code>def execute(self, input_data: str) -&gt; dict:\n    return {\n        \"result\": processed_data,\n        \"metadata\": {\n            \"processed_at\": datetime.now().isoformat(),\n            \"input_size\": len(input_data),\n            \"processing_time\": elapsed_time\n        }\n    }\n</code></pre>"},{"location":"plugins/#4-error-messages","title":"4. Error Messages","text":"<p>Provide clear, actionable error messages:</p> <pre><code>def execute(self, file_path: str) -&gt; dict:\n    if not os.path.exists(file_path):\n        raise FileNotFoundError(\n            f\"File not found: {file_path}. \"\n            f\"Please check the path and ensure the file exists.\"\n        )\n</code></pre>"},{"location":"plugins/#5-documentation","title":"5. Documentation","text":"<p>Document your plugins with docstrings:</p> <pre><code>class EmailPlugin(aroflow.PluginMixin):\n    \"\"\"Send emails using SMTP.\n\n    This plugin sends emails through an SMTP server. It supports\n    both plain text and HTML messages.\n    \"\"\"\n\n    plugin_name = \"send_email\"\n\n    def execute(\n        self, \n        to: str, \n        subject: str, \n        body: str,\n        html: bool = False\n    ) -&gt; dict:\n        \"\"\"Send an email.\n\n        Args:\n            to: Recipient email address\n            subject: Email subject line\n            body: Email body content\n            html: Whether body contains HTML (default: False)\n\n        Returns:\n            dict: Sending status and metadata\n\n        Raises:\n            ValueError: If email address is invalid\n            ConnectionError: If SMTP server is unreachable\n        \"\"\"\n        # Implementation here\n        pass\n</code></pre>"},{"location":"plugins/#testing-plugins","title":"Testing Plugins","text":""},{"location":"plugins/#unit-testing","title":"Unit Testing","text":"<p>Test your plugins independently:</p> <pre><code>import pytest\nfrom my_plugins import DataProcessorPlugin\n\ndef test_data_processor_upper():\n    plugin = DataProcessorPlugin()\n    result = plugin.execute(\"hello world\", \"upper\")\n    assert result == \"HELLO WORLD\"\n\ndef test_data_processor_invalid_transform():\n    plugin = DataProcessorPlugin()\n    with pytest.raises(ValueError):\n        plugin.execute(\"hello\", \"invalid_transform\")\n</code></pre>"},{"location":"plugins/#integration-testing","title":"Integration Testing","text":"<p>Test plugins within workflows:</p> <pre><code>def test_plugin_in_workflow():\n    client = aroflow.create(aroflow.BackendType.IN_MEMORY)\n    client.plugin(DataProcessorPlugin)\n\n    workflow = {\n        \"steps\": [{\n            \"id\": \"step1\",\n            \"kind\": \"operation\",\n            \"operation\": \"process_data\",\n            \"parameters\": {\"data\": \"test\", \"transform\": \"upper\"}\n        }]\n    }\n\n    result = client.run(workflow)\n    assert result.status == \"success\"\n    assert result.results[0].result == \"TEST\"\n</code></pre> <p>Next: Learn how to compose these plugins into powerful workflows!</p>"},{"location":"workflows/","title":"Creating Workflows","text":"<p>Workflows are the orchestration layer of AroFlow. They define the sequence and structure of operations to achieve your goals. This guide covers everything from simple linear workflows to complex parallel and data processing pipelines.</p>"},{"location":"workflows/#workflow-basics","title":"Workflow Basics","text":""},{"location":"workflows/#what-is-a-workflow","title":"What is a Workflow?","text":"<p>A workflow is a JSON/dictionary structure that defines a series of steps to execute. Each step specifies what operation to perform and how it connects to other steps.</p> <pre><code>workflow = {\n    \"steps\": [\n        {\n            \"id\": \"step1\",           # Unique identifier\n            \"kind\": \"operation\",     # Step type\n            \"operation\": \"my_plugin\", # Plugin to execute\n            \"parameters\": {          # Input parameters\n                \"param1\": \"value1\"\n            }\n        }\n    ]\n}\n</code></pre>"},{"location":"workflows/#step-types","title":"Step Types","text":"<p>AroFlow supports three types of steps:</p> <ol> <li>Operation Steps - Execute a single plugin</li> <li>Map Steps - Execute a plugin over a list of inputs</li> <li>Parallel Steps - Execute multiple operations concurrently</li> </ol>"},{"location":"workflows/#operation-steps","title":"Operation Steps","text":"<p>Operation steps execute a single plugin with specified parameters.</p>"},{"location":"workflows/#basic-operation","title":"Basic Operation","text":"<pre><code>{\n    \"id\": \"send_notification\",\n    \"kind\": \"operation\",\n    \"operation\": \"send_email\",\n    \"parameters\": {\n        \"to\": \"user@example.com\",\n        \"subject\": \"Workflow Complete\",\n        \"body\": \"Your data processing is complete!\"\n    }\n}\n</code></pre>"},{"location":"workflows/#error-handling-options","title":"Error Handling Options","text":"<pre><code>{\n    \"id\": \"risky_operation\",\n    \"kind\": \"operation\",\n    \"operation\": \"external_api_call\",\n    \"parameters\": {\n        \"url\": \"https://api.example.com/data\"\n    },\n    \"retries\": 3,              # Retry failed operations\n    \"timeout\": 30.0,           # Timeout in seconds\n    \"fail_workflow\": False     # Continue on failure\n}\n</code></pre>"},{"location":"workflows/#map-steps","title":"Map Steps","text":"<p>Map steps execute a plugin over a list of inputs, perfect for data processing pipelines.</p>"},{"location":"workflows/#sequential-map","title":"Sequential Map","text":"<p>Process items one after another:</p> <pre><code>{\n    \"id\": \"process_files\",\n    \"kind\": \"map\",\n    \"mode\": \"sequential\",      # Process one at a time\n    \"iterator\": \"file_path\",   # Variable name for current item\n    \"inputs\": [                # List of items to process\n        \"/data/file1.txt\",\n        \"/data/file2.txt\", \n        \"/data/file3.txt\"\n    ],\n    \"operation\": {\n        \"id\": \"file_processor\",\n        \"kind\": \"operation\",\n        \"operation\": \"process_file\",\n        \"parameters\": {\n            \"file_path\": \"${file_path}\",  # Reference current item\n            \"output_dir\": \"/processed/\"\n        }\n    }\n}\n</code></pre>"},{"location":"workflows/#parallel-map","title":"Parallel Map","text":"<p>Process items concurrently for better performance:</p> <pre><code>{\n    \"id\": \"resize_images\",\n    \"kind\": \"map\",\n    \"mode\": \"parallel\",        # Process concurrently\n    \"iterator\": \"image\",\n    \"inputs\": [\n        {\"path\": \"img1.jpg\", \"size\": \"800x600\"},\n        {\"path\": \"img2.jpg\", \"size\": \"800x600\"},\n        {\"path\": \"img3.jpg\", \"size\": \"800x600\"}\n    ],\n    \"operation\": {\n        \"id\": \"image_resizer\",\n        \"kind\": \"operation\",\n        \"operation\": \"resize_image\",\n        \"parameters\": {\n            \"input_path\": \"${image.path}\",\n            \"output_size\": \"${image.size}\",\n            \"quality\": 85\n        }\n    }\n}\n</code></pre>"},{"location":"workflows/#map-with-dynamic-inputs","title":"Map with Dynamic Inputs","text":"<p>Use results from previous steps as map inputs:</p> <pre><code>{\n    \"steps\": [\n        {\n            \"id\": \"get_file_list\",\n            \"kind\": \"operation\",\n            \"operation\": \"list_files\",\n            \"parameters\": {\"directory\": \"/uploads/\"}\n        },\n        {\n            \"id\": \"process_all_files\", \n            \"kind\": \"map\",\n            \"mode\": \"parallel\",\n            \"iterator\": \"file\",\n            \"inputs\": \"${get_file_list.result}\",  # Use previous result\n            \"operation\": {\n                \"id\": \"file_processor\",\n                \"kind\": \"operation\",\n                \"operation\": \"process_file\",\n                \"parameters\": {\n                    \"file_path\": \"${file}\"\n                }\n            }\n        }\n    ]\n}\n</code></pre>"},{"location":"workflows/#parallel-steps","title":"Parallel Steps","text":"<p>Parallel steps execute multiple operations concurrently.</p>"},{"location":"workflows/#basic-parallel-execution","title":"Basic Parallel Execution","text":"<pre><code>{\n    \"id\": \"parallel_notifications\",\n    \"kind\": \"parallel\",\n    \"operations\": [\n        {\n            \"id\": \"email_notification\",\n            \"kind\": \"operation\",\n            \"operation\": \"send_email\",\n            \"parameters\": {\n                \"to\": \"admin@company.com\",\n                \"subject\": \"Process Complete\"\n            }\n        },\n        {\n            \"id\": \"slack_notification\", \n            \"kind\": \"operation\",\n            \"operation\": \"send_slack\",\n            \"parameters\": {\n                \"channel\": \"#alerts\",\n                \"message\": \"Data processing finished\"\n            }\n        },\n        {\n            \"id\": \"update_dashboard\",\n            \"kind\": \"operation\", \n            \"operation\": \"update_status\",\n            \"parameters\": {\n                \"status\": \"complete\",\n                \"timestamp\": \"${workflow.start_time}\"\n            }\n        }\n    ]\n}\n</code></pre>"},{"location":"workflows/#parallel-data-processing","title":"Parallel Data Processing","text":"<pre><code>{\n    \"id\": \"parallel_analysis\",\n    \"kind\": \"parallel\",\n    \"operations\": [\n        {\n            \"id\": \"calculate_stats\",\n            \"kind\": \"operation\",\n            \"operation\": \"statistical_analysis\",\n            \"parameters\": {\n                \"data\": \"${previous_step.result}\",\n                \"analysis_type\": \"descriptive\"\n            }\n        },\n        {\n            \"id\": \"generate_charts\",\n            \"kind\": \"operation\",\n            \"operation\": \"create_visualization\",\n            \"parameters\": {\n                \"data\": \"${previous_step.result}\",\n                \"chart_type\": \"histogram\"\n            }\n        },\n        {\n            \"id\": \"export_data\",\n            \"kind\": \"operation\",\n            \"operation\": \"export_csv\",\n            \"parameters\": {\n                \"data\": \"${previous_step.result}\",\n                \"filename\": \"analysis_results.csv\"\n            }\n        }\n    ]\n}\n</code></pre>"},{"location":"workflows/#variable-substitution","title":"Variable Substitution","text":"<p>AroFlow supports powerful variable substitution to pass data between steps.</p>"},{"location":"workflows/#basic-variable-references","title":"Basic Variable References","text":"<pre><code>{\n    \"steps\": [\n        {\n            \"id\": \"get_user_data\",\n            \"kind\": \"operation\",\n            \"operation\": \"fetch_user\",\n            \"parameters\": {\"user_id\": \"12345\"}\n        },\n        {\n            \"id\": \"send_welcome\",\n            \"kind\": \"operation\", \n            \"operation\": \"send_email\",\n            \"parameters\": {\n                \"to\": \"${get_user_data.result.email}\",      # Access nested fields\n                \"subject\": \"Welcome ${get_user_data.result.name}!\",\n                \"body\": \"Hello ${get_user_data.result.name}, welcome aboard!\"\n            }\n        }\n    ]\n}\n</code></pre>"},{"location":"workflows/#array-access","title":"Array Access","text":"<pre><code>{\n    \"steps\": [\n        {\n            \"id\": \"get_file_list\",\n            \"kind\": \"operation\",\n            \"operation\": \"list_files\",\n            \"parameters\": {\"directory\": \"/data/\"}\n        },\n        {\n            \"id\": \"process_first_file\",\n            \"kind\": \"operation\",\n            \"operation\": \"process_file\", \n            \"parameters\": {\n                \"file_path\": \"${get_file_list.result[0]}\",  # First file\n                \"backup_path\": \"${get_file_list.result[1]}\" # Second file\n            }\n        }\n    ]\n}\n</code></pre>"},{"location":"workflows/#complex-variable-expressions","title":"Complex Variable Expressions","text":"<pre><code>{\n    \"id\": \"conditional_processing\",\n    \"kind\": \"operation\",\n    \"operation\": \"process_data\",\n    \"parameters\": {\n        \"input_data\": \"${data_validation.result.cleaned_data}\",\n        \"config\": {\n            \"batch_size\": \"${data_validation.result.record_count}\",\n            \"parallel\": \"${system_info.result.cpu_count}\",\n            \"memory_limit\": \"${system_info.result.available_memory}\"\n        }\n    }\n}\n</code></pre>"},{"location":"workflows/#complete-workflow-examples","title":"Complete Workflow Examples","text":""},{"location":"workflows/#data-processing-pipeline","title":"Data Processing Pipeline","text":"<pre><code>data_pipeline = {\n    \"steps\": [\n        # Step 1: Validate input data\n        {\n            \"id\": \"validate_input\",\n            \"kind\": \"operation\",\n            \"operation\": \"data_validator\",\n            \"parameters\": {\n                \"file_path\": \"/input/raw_data.csv\",\n                \"schema\": {\n                    \"required_columns\": [\"id\", \"name\", \"email\", \"age\"],\n                    \"data_types\": {\"id\": \"int\", \"age\": \"int\"}\n                }\n            }\n        },\n\n        # Step 2: Clean data in parallel\n        {\n            \"id\": \"parallel_cleaning\",\n            \"kind\": \"parallel\", \n            \"operations\": [\n                {\n                    \"id\": \"remove_duplicates\",\n                    \"kind\": \"operation\",\n                    \"operation\": \"deduplicator\",\n                    \"parameters\": {\n                        \"data\": \"${validate_input.result.data}\",\n                        \"key_columns\": [\"email\"]\n                    }\n                },\n                {\n                    \"id\": \"normalize_text\",\n                    \"kind\": \"operation\",\n                    \"operation\": \"text_normalizer\",\n                    \"parameters\": {\n                        \"data\": \"${validate_input.result.data}\",\n                        \"columns\": [\"name\"],\n                        \"operations\": [\"trim\", \"title_case\"]\n                    }\n                }\n            ]\n        },\n\n        # Step 3: Process each record\n        {\n            \"id\": \"process_records\",\n            \"kind\": \"map\",\n            \"mode\": \"parallel\",\n            \"iterator\": \"record\",\n            \"inputs\": \"${remove_duplicates.result}\",\n            \"operation\": {\n                \"id\": \"record_processor\",\n                \"kind\": \"operation\",\n                \"operation\": \"enrich_record\",\n                \"parameters\": {\n                    \"record\": \"${record}\",\n                    \"enrichment_api\": \"https://api.enrichment.com/v1/person\"\n                }\n            }\n        },\n\n        # Step 4: Generate outputs\n        {\n            \"id\": \"generate_outputs\",\n            \"kind\": \"parallel\",\n            \"operations\": [\n                {\n                    \"id\": \"save_to_database\",\n                    \"kind\": \"operation\",\n                    \"operation\": \"database_writer\",\n                    \"parameters\": {\n                        \"data\": \"${process_records.results}\",\n                        \"table\": \"processed_customers\",\n                        \"mode\": \"replace\"\n                    }\n                },\n                {\n                    \"id\": \"generate_report\",\n                    \"kind\": \"operation\",\n                    \"operation\": \"report_generator\",\n                    \"parameters\": {\n                        \"data\": \"${process_records.results}\",\n                        \"template\": \"customer_analysis.html\",\n                        \"output_path\": \"/reports/customer_report.html\"\n                    }\n                },\n                {\n                    \"id\": \"send_notification\",\n                    \"kind\": \"operation\",\n                    \"operation\": \"send_email\",\n                    \"parameters\": {\n                        \"to\": \"data-team@company.com\",\n                        \"subject\": \"Data Processing Complete\",\n                        \"body\": \"Processed ${process_records.results.length} records successfully.\"\n                    }\n                }\n            ]\n        }\n    ]\n}\n</code></pre>"},{"location":"workflows/#ml-model-training-pipeline","title":"ML Model Training Pipeline","text":"<pre><code>ml_pipeline = {\n    \"steps\": [\n        # Data preparation\n        {\n            \"id\": \"load_dataset\",\n            \"kind\": \"operation\",\n            \"operation\": \"dataset_loader\",\n            \"parameters\": {\n                \"source\": \"s3://ml-bucket/training-data/\",\n                \"format\": \"parquet\"\n            }\n        },\n\n        # Feature engineering in parallel\n        {\n            \"id\": \"feature_engineering\",\n            \"kind\": \"parallel\",\n            \"operations\": [\n                {\n                    \"id\": \"numerical_features\",\n                    \"kind\": \"operation\",\n                    \"operation\": \"numerical_preprocessor\", \n                    \"parameters\": {\n                        \"data\": \"${load_dataset.result}\",\n                        \"columns\": [\"age\", \"income\", \"credit_score\"],\n                        \"scaling\": \"standard\"\n                    }\n                },\n                {\n                    \"id\": \"categorical_features\",\n                    \"kind\": \"operation\",\n                    \"operation\": \"categorical_preprocessor\",\n                    \"parameters\": {\n                        \"data\": \"${load_dataset.result}\",\n                        \"columns\": [\"category\", \"region\", \"product_type\"],\n                        \"encoding\": \"one_hot\"\n                    }\n                },\n                {\n                    \"id\": \"text_features\",\n                    \"kind\": \"operation\",\n                    \"operation\": \"text_vectorizer\",\n                    \"parameters\": {\n                        \"data\": \"${load_dataset.result}\",\n                        \"text_column\": \"description\",\n                        \"method\": \"tfidf\",\n                        \"max_features\": 1000\n                    }\n                }\n            ]\n        },\n\n        # Combine features\n        {\n            \"id\": \"combine_features\",\n            \"kind\": \"operation\",\n            \"operation\": \"feature_combiner\",\n            \"parameters\": {\n                \"numerical\": \"${numerical_features.result}\",\n                \"categorical\": \"${categorical_features.result}\", \n                \"text\": \"${text_features.result}\"\n            }\n        },\n\n        # Train multiple models in parallel\n        {\n            \"id\": \"train_models\",\n            \"kind\": \"map\",\n            \"mode\": \"parallel\",\n            \"iterator\": \"model_config\",\n            \"inputs\": [\n                {\"name\": \"random_forest\", \"params\": {\"n_estimators\": 100, \"max_depth\": 10}},\n                {\"name\": \"gradient_boosting\", \"params\": {\"learning_rate\": 0.1, \"n_estimators\": 100}},\n                {\"name\": \"logistic_regression\", \"params\": {\"C\": 1.0, \"max_iter\": 1000}}\n            ],\n            \"operation\": {\n                \"id\": \"model_trainer\",\n                \"kind\": \"operation\",\n                \"operation\": \"train_model\",\n                \"parameters\": {\n                    \"features\": \"${combine_features.result.features}\",\n                    \"target\": \"${combine_features.result.target}\",\n                    \"model_type\": \"${model_config.name}\",\n                    \"hyperparameters\": \"${model_config.params}\"\n                }\n            }\n        },\n\n        # Evaluate and select best model\n        {\n            \"id\": \"model_evaluation\",\n            \"kind\": \"operation\",\n            \"operation\": \"model_evaluator\",\n            \"parameters\": {\n                \"models\": \"${train_models.results}\",\n                \"test_data\": \"${combine_features.result.test_set}\",\n                \"metrics\": [\"accuracy\", \"precision\", \"recall\", \"f1_score\"]\n            }\n        },\n\n        # Deploy best model\n        {\n            \"id\": \"deploy_model\",\n            \"kind\": \"operation\",\n            \"operation\": \"model_deployer\",\n            \"parameters\": {\n                \"model\": \"${model_evaluation.result.best_model}\",\n                \"endpoint\": \"prod-ml-api\",\n                \"version\": \"v1.0\"\n            }\n        }\n    ]\n}\n</code></pre>"},{"location":"workflows/#etl-pipeline-with-error-handling","title":"ETL Pipeline with Error Handling","text":"<pre><code>etl_pipeline = {\n    \"steps\": [\n        # Extract from multiple sources\n        {\n            \"id\": \"extract_data\",\n            \"kind\": \"parallel\",\n            \"operations\": [\n                {\n                    \"id\": \"extract_database\",\n                    \"kind\": \"operation\",\n                    \"operation\": \"database_extractor\",\n                    \"parameters\": {\n                        \"connection\": \"postgresql://prod-db:5432/sales\",\n                        \"query\": \"SELECT * FROM orders WHERE created_at &gt;= NOW() - INTERVAL '1 day'\"\n                    },\n                    \"retries\": 3,\n                    \"timeout\": 300.0\n                },\n                {\n                    \"id\": \"extract_api\",\n                    \"kind\": \"operation\", \n                    \"operation\": \"api_extractor\",\n                    \"parameters\": {\n                        \"url\": \"https://api.partner.com/v1/transactions\",\n                        \"headers\": {\"Authorization\": \"Bearer ${env.API_TOKEN}\"},\n                        \"params\": {\"since\": \"${workflow.start_time}\"}\n                    },\n                    \"retries\": 5,\n                    \"timeout\": 60.0,\n                    \"fail_workflow\": False  # Continue even if API fails\n                },\n                {\n                    \"id\": \"extract_files\",\n                    \"kind\": \"map\",\n                    \"mode\": \"sequential\",\n                    \"iterator\": \"file_pattern\",\n                    \"inputs\": [\n                        \"/data/csv/*.csv\",\n                        \"/data/json/*.json\", \n                        \"/data/xml/*.xml\"\n                    ],\n                    \"operation\": {\n                        \"id\": \"file_extractor\",\n                        \"kind\": \"operation\",\n                        \"operation\": \"file_reader\",\n                        \"parameters\": {\n                            \"pattern\": \"${file_pattern}\",\n                            \"format\": \"auto\"\n                        }\n                    }\n                }\n            ]\n        },\n\n        # Transform data\n        {\n            \"id\": \"transform_data\",\n            \"kind\": \"operation\",\n            \"operation\": \"data_transformer\",\n            \"parameters\": {\n                \"database_data\": \"${extract_database.result}\",\n                \"api_data\": \"${extract_api.result}\",\n                \"file_data\": \"${extract_files.results}\",\n                \"transformations\": [\n                    \"standardize_dates\",\n                    \"normalize_currency\", \n                    \"deduplicate_records\",\n                    \"validate_business_rules\"\n                ]\n            }\n        },\n\n        # Load to destinations  \n        {\n            \"id\": \"load_data\",\n            \"kind\": \"parallel\",\n            \"operations\": [\n                {\n                    \"id\": \"load_warehouse\",\n                    \"kind\": \"operation\",\n                    \"operation\": \"warehouse_loader\",\n                    \"parameters\": {\n                        \"data\": \"${transform_data.result}\",\n                        \"destination\": \"snowflake://warehouse/sales_db\",\n                        \"table\": \"daily_transactions\",\n                        \"mode\": \"append\"\n                    }\n                },\n                {\n                    \"id\": \"load_analytics\",\n                    \"kind\": \"operation\",\n                    \"operation\": \"analytics_loader\", \n                    \"parameters\": {\n                        \"data\": \"${transform_data.result}\",\n                        \"destination\": \"bigquery://analytics-project/sales_dataset\",\n                        \"partition_by\": \"transaction_date\"\n                    }\n                },\n                {\n                    \"id\": \"update_cache\",\n                    \"kind\": \"operation\",\n                    \"operation\": \"cache_updater\",\n                    \"parameters\": {\n                        \"data\": \"${transform_data.result}\",\n                        \"cache_key\": \"daily_sales_summary\",\n                        \"ttl\": 86400\n                    }\n                }\n            ]\n        },\n\n        # Quality checks and notifications\n        {\n            \"id\": \"quality_checks\",\n            \"kind\": \"operation\",\n            \"operation\": \"data_quality_checker\",\n            \"parameters\": {\n                \"source_count\": \"${extract_database.result.count}\",\n                \"transformed_count\": \"${transform_data.result.count}\",\n                \"loaded_count\": \"${load_warehouse.result.count}\",\n                \"quality_rules\": [\n                    \"row_count_tolerance: 0.05\",\n                    \"null_percentage_max: 0.1\",\n                    \"duplicate_percentage_max: 0.01\"\n                ]\n            }\n        },\n\n        # Send completion notification\n        {\n            \"id\": \"send_notification\",\n            \"kind\": \"operation\",\n            \"operation\": \"send_slack\",\n            \"parameters\": {\n                \"channel\": \"#data-engineering\",\n                \"message\": {\n                    \"text\": \"ETL Pipeline Complete\",\n                    \"blocks\": [\n                        {\n                            \"type\": \"section\",\n                            \"text\": {\n                                \"type\": \"mrkdwn\",\n                                \"text\": \"*ETL Pipeline Results*\\n\u2022 Extracted: ${extract_database.result.count} records\\n\u2022 Transformed: ${transform_data.result.count} records\\n\u2022 Loaded: ${load_warehouse.result.count} records\\n\u2022 Quality Score: ${quality_checks.result.score}%\"\n                            }\n                        }\n                    ]\n                }\n            }\n        }\n    ]\n}\n</code></pre>"},{"location":"workflows/#best-practices","title":"Best Practices","text":""},{"location":"workflows/#1-step-naming","title":"1. Step Naming","text":"<p>Use descriptive step IDs that explain what the step does:</p> <pre><code># Good\n\"id\": \"validate_customer_data\"\n\"id\": \"send_welcome_email\"  \n\"id\": \"calculate_monthly_revenue\"\n\n# Avoid\n\"id\": \"step1\"\n\"id\": \"process\"\n\"id\": \"do_stuff\"\n</code></pre>"},{"location":"workflows/#2-error-handling-strategy","title":"2. Error Handling Strategy","text":"<p>Plan for failures with appropriate retry and timeout settings:</p> <pre><code>{\n    \"id\": \"critical_step\",\n    \"kind\": \"operation\",\n    \"operation\": \"important_operation\",\n    \"parameters\": {...},\n    \"retries\": 3,           # Retry failed operations\n    \"timeout\": 120.0,       # Prevent hanging\n    \"fail_workflow\": True   # Stop entire workflow on failure\n}\n\n{\n    \"id\": \"optional_step\", \n    \"kind\": \"operation\",\n    \"operation\": \"nice_to_have_operation\",\n    \"parameters\": {...},\n    \"fail_workflow\": False  # Continue workflow even if this fails\n}\n</code></pre>"},{"location":"workflows/#3-efficient-parallelization","title":"3. Efficient Parallelization","text":"<p>Use parallel processing where operations are independent:</p> <pre><code># Process independent operations in parallel\n{\n    \"id\": \"parallel_notifications\",\n    \"kind\": \"parallel\",\n    \"operations\": [\n        {\"id\": \"email\", \"kind\": \"operation\", \"operation\": \"send_email\", ...},\n        {\"id\": \"slack\", \"kind\": \"operation\", \"operation\": \"send_slack\", ...},\n        {\"id\": \"webhook\", \"kind\": \"operation\", \"operation\": \"send_webhook\", ...}\n    ]\n}\n\n# Use parallel map for data processing\n{\n    \"id\": \"process_files\",\n    \"kind\": \"map\", \n    \"mode\": \"parallel\",  # Much faster than sequential\n    \"iterator\": \"file\",\n    \"inputs\": [\"file1.txt\", \"file2.txt\", \"file3.txt\"],\n    \"operation\": {...}\n}\n</code></pre>"},{"location":"workflows/#4-clear-data-flow","title":"4. Clear Data Flow","text":"<p>Make data dependencies explicit with meaningful variable names:</p> <pre><code>{\n    \"steps\": [\n        {\n            \"id\": \"extract_customer_data\",\n            \"kind\": \"operation\",\n            \"operation\": \"database_query\",\n            \"parameters\": {\"query\": \"SELECT * FROM customers\"}\n        },\n        {\n            \"id\": \"enrich_customer_profiles\",\n            \"kind\": \"map\",\n            \"mode\": \"parallel\",\n            \"iterator\": \"customer\",\n            \"inputs\": \"${extract_customer_data.result.rows}\",  # Clear data source\n            \"operation\": {\n                \"id\": \"customer_enrichment\",\n                \"kind\": \"operation\", \n                \"operation\": \"enrich_customer\",\n                \"parameters\": {\n                    \"customer_id\": \"${customer.id}\",          # Clear field access\n                    \"customer_email\": \"${customer.email}\"\n                }\n            }\n        }\n    ]\n}\n</code></pre>"},{"location":"workflows/#5-workflow-documentation","title":"5. Workflow Documentation","text":"<p>Document complex workflows with comments:</p> <pre><code>complex_workflow = {\n    \"steps\": [\n        {\n            # Extract data from multiple sources in parallel for efficiency\n            \"id\": \"parallel_extraction\", \n            \"kind\": \"parallel\",\n            \"operations\": [...]\n        },\n        {\n            # Transform extracted data with validation and enrichment\n            \"id\": \"data_transformation\",\n            \"kind\": \"operation\", \n            \"operation\": \"transform_pipeline\",\n            \"parameters\": {\n                # Apply business rules and data quality checks\n                \"validation_rules\": [...],\n                \"enrichment_apis\": [...]\n            }\n        }\n    ]\n}\n</code></pre>"},{"location":"workflows/#running-workflows","title":"Running Workflows","text":""},{"location":"workflows/#execute-workflow","title":"Execute Workflow","text":"<pre><code>import aroflow\n\n# Create client and register plugins\nclient = aroflow.create(aroflow.BackendType.IN_MEMORY)\nclient.plugin(DataProcessorPlugin)\nclient.plugin(EmailPlugin)\n\n# Run workflow\nresult = client.run(workflow)\n\n# Check results\nif result.status == \"success\":\n    print(\"Workflow completed successfully!\")\n    for step_result in result.results:\n        print(f\"Step {step_result.id}: {step_result.status}\")\nelse:\n    print(f\"Workflow failed: {result.error}\")\n</code></pre>"},{"location":"workflows/#with-custom-workflow-id","title":"With Custom Workflow ID","text":"<pre><code>result = client.run(workflow, workflow_id=\"data_pipeline_2024_01_15\")\n</code></pre>"},{"location":"workflows/#examining-results","title":"Examining Results","text":"<pre><code># Get workflow results in different formats\nresult_dict = result.to_dict()\nresult_json = result.to_json()\nresult_yaml = result.to_yaml()\n\n# Access specific step results\nfirst_step = result.results[0]\nprint(f\"Step: {first_step.id}\")\nprint(f\"Status: {first_step.status}\")\nprint(f\"Result: {first_step.result}\")\n\n# Find step by ID\ndef find_step_result(results, step_id):\n    return next((r for r in results if r.id == step_id), None)\n\nemail_result = find_step_result(result.results, \"send_notification\")\nif email_result:\n    print(f\"Email sent: {email_result.result}\")\n</code></pre> <p>Next: Explore comprehensive examples and real-world use cases!</p>"},{"location":"blog/","title":"Blog","text":""},{"location":"blog/2025/08/29/hello-world/","title":"Hello world!","text":"<p>Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa.</p>","tags":["Foo","Bar"]},{"location":"blog/archive/2025/","title":"2025","text":""},{"location":"blog/category/hello/","title":"Hello","text":""},{"location":"blog/category/world/","title":"World","text":""}]}